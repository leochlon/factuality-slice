{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Factuality Slice: Evidence-Grounded QA Training Pipeline\n",
        "\n",
        "This notebook implements a complete training pipeline for evidence-grounded question answering with citation training.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "1. **Data Building**: Create unified dataset from FEVER, HotpotQA, NQ-Open, and PopQA\n",
        "2. **Supervised Fine-Tuning (SFT)**: Train base model to answer questions with citations\n",
        "3. **Evaluation**: Compare SFT model against base model\n",
        "4. **Preference Generation**: Create preference pairs using judge model\n",
        "5. **Reward Model Training**: Train reward model with anti-gaming features\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Google Colab with A100 GPU (recommended)\n",
        "- Hugging Face account for model downloads\n",
        "- ~50GB storage for models and data"
      ],
      "metadata": {
        "id": "notebook-overview"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Environment Setup\n",
        "\n",
        "Mount Google Drive for persistent storage and authenticate with Hugging Face for model access."
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Login to Hugging Face (you'll need to paste your access token)\n",
        "# Get your token from: https://huggingface.co/settings/tokens\n",
        "!huggingface-cli login\n",
        "\n",
        "# Install required packages\n",
        "!pip install -U bitsandbytes transformers peft accelerate datasets\n",
        "!pip install scipy scikit-learn pyyaml tqdm"
      ],
      "metadata": {
        "id": "1rxtatCssawU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Build the Dataset\n",
        "\n",
        "Download and process data from multiple QA sources. This creates:\n",
        "- `data/processed/train.jsonl`: Training data\n",
        "- `data/processed/val.jsonl`: Validation data\n",
        "- `data/processed/test.jsonl`: Test data\n",
        "\n",
        "Each sample contains:\n",
        "- Question\n",
        "- Evidence chunks (context)\n",
        "- Gold answer\n",
        "- Support spans (which evidence chunks contain the answer)\n",
        "- Hard negatives (topically related but incorrect evidence)"
      ],
      "metadata": {
        "id": "data-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFTiZUE3MKHL"
      },
      "outputs": [],
      "source": [
        "# Build the unified dataset from multiple sources\n",
        "# --accept-new-hash: Required on first run to create source verification hashes\n",
        "# --seed: Ensures reproducible train/val/test splits\n",
        "!python build_data.py --data-dir data --seed 42 --accept-new-hash\n",
        "\n",
        "# Check the generated data\n",
        "!echo \"Dataset statistics:\"\n",
        "!wc -l data/processed/*.jsonl\n",
        "!echo \"\\nSample from training data:\"\n",
        "!head -n 1 data/processed/train.jsonl | python -m json.tool | head -20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Generate Prompt Templates\n",
        "\n",
        "Create standardized prompt templates and output schemas. This ensures consistent formatting across training and inference."
      ],
      "metadata": {
        "id": "prompt-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompt templates and validation schemas\n",
        "!python prompt_schema.py save --prompts-dir prompts\n",
        "\n",
        "# View the generated templates\n",
        "!echo \"Generated prompt templates:\"\n",
        "!ls -la prompts/\n",
        "\n",
        "# Test the output validator\n",
        "!python prompt_schema.py test"
      ],
      "metadata": {
        "id": "zersoDvLMsSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "Fine-tune a base model using LoRA/QLoRA for parameter-efficient training.\n",
        "\n",
        "### Training Details:\n",
        "- **Method**: QLoRA (4-bit quantization with LoRA adapters)\n",
        "- **Base Model**: Gemma-2-9B (configurable)\n",
        "- **Training Objective**: Next-token prediction on JSON answers\n",
        "- **Key Features**:\n",
        "  - Answer-only loss (prompt tokens masked)\n",
        "  - Gradient checkpointing for memory efficiency\n",
        "  - Mixed precision training (bf16)\n",
        "\n",
        "### Expected Training Time:\n",
        "- A100 GPU: ~2-3 hours for 2 epochs\n",
        "- V100 GPU: ~4-5 hours for 2 epochs"
      ],
      "metadata": {
        "id": "sft-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create configuration file for SFT training\n",
        "# You can modify these parameters based on your needs\n",
        "!mkdir -p configs\n",
        "config_content = \"\"\"model_name: google/gemma-2-9b\n",
        "output_dir: checkpoints/sft/gemma2-9b\n",
        "load_in_4bit: true\n",
        "bf16: true\n",
        "trust_remote_code: false\n",
        "attn_implementation: eager  # Use 'flash_attention_2' if available\n",
        "\n",
        "lora:\n",
        "  r: 16\n",
        "  alpha: 32\n",
        "  dropout: 0.05\n",
        "  target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]\n",
        "\n",
        "training:\n",
        "  per_device_train_batch_size: 1\n",
        "  per_device_eval_batch_size: 2\n",
        "  gradient_accumulation_steps: 64\n",
        "  learning_rate: 1.5e-5\n",
        "  num_train_epochs: 2\n",
        "  max_seq_length: 2048\n",
        "  warmup_ratio: 0.03\n",
        "  weight_decay: 0.1\n",
        "  lr_scheduler_type: cosine\n",
        "  logging_steps: 10\n",
        "  eval_steps: 100\n",
        "  save_steps: 100\n",
        "  save_total_limit: 2\n",
        "  seed: 42\n",
        "\n",
        "data:\n",
        "  train_path: data/processed/train.jsonl\n",
        "  val_path: data/processed/val.jsonl\n",
        "  template: default\n",
        "  max_samples: null  # Use all training data\n",
        "  max_val_samples: 500\n",
        "\"\"\"\n",
        "\n",
        "with open('configs/sft_gemma2.yaml', 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "# Train the SFT model\n",
        "!python train_sft.py -c configs/sft_gemma2.yaml"
      ],
      "metadata": {
        "id": "-QoDRI3fPM66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Evaluate SFT Model\n",
        "\n",
        "Compare the fine-tuned model against the base model on test data.\n",
        "\n",
        "### Metrics Evaluated:\n",
        "- **Exact Match (EM)**: Percentage of exact answer matches\n",
        "- **F1 Score**: Token-level overlap with gold answers\n",
        "- **Hallucination Rate**: Answers without proper evidence citations\n",
        "- **Citation Correctness**: Whether citations match gold support spans\n",
        "- **Refusal Rate**: Frequency of \"insufficient evidence\" responses"
      ],
      "metadata": {
        "id": "eval-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best checkpoint (usually the last one)\n",
        "!ls -la checkpoints/sft/gemma2-9b/\n",
        "\n",
        "# Evaluate the SFT model and compare with base model\n",
        "# Note: Adjust checkpoint number based on training output\n",
        "!python evaluate_sft_fast_robust.py \\\n",
        "  --model-path checkpoints/sft/gemma2-9b/checkpoint-35 \\\n",
        "  --base-model google/gemma-2-9b \\\n",
        "  --test-data data/processed/test.jsonl \\\n",
        "  --compare-baseline \\\n",
        "  --batch-size 4 \\\n",
        "  --max-new-tokens 256 \\\n",
        "  --fast \\\n",
        "  --buckets 2048,3072,4096,5120,6144,7168,7936 \\\n",
        "  --attn-impl flash2 \\\n",
        "  --output eval_results.json\n",
        "\n",
        "# Display evaluation results\n",
        "!echo \"\\nEvaluation Results:\"\n",
        "!cat eval_results.json | python -m json.tool"
      ],
      "metadata": {
        "id": "kuAMim8TPQGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Generate Preference Data (RLAIF)\n",
        "\n",
        "Create preference pairs for reward model training using a two-phase approach:\n",
        "\n",
        "### Phase 1: Generate Candidates\n",
        "- Generate multiple responses from both SFT and base models\n",
        "- Responses are cached for efficiency\n",
        "\n",
        "### Phase 2: Judge Candidates\n",
        "- Use a separate judge model to evaluate response pairs\n",
        "- Create preference pairs based on judge scores\n",
        "- Filter pairs by minimum margin for quality\n",
        "\n",
        "This two-phase approach allows:\n",
        "- Memory-efficient processing (only one model loaded at a time)\n",
        "- Resume capability if interrupted\n",
        "- Different quantization settings for generators vs judge"
      ],
      "metadata": {
        "id": "prefs-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 1: Generate candidate responses from SFT and base models\n",
        "# This creates cached generations in prefs/cache/<signature>/\n",
        "!python make_prefs.py --phase generate \\\n",
        "  --sft-model checkpoints/sft/gemma2-9b/checkpoint-29 \\\n",
        "  --base-model google/gemma-2-9b \\\n",
        "  --data-path data/processed/train.jsonl \\\n",
        "  --max-samples 600 \\\n",
        "  --n-generations 2 \\\n",
        "  --batch-size 12 \\\n",
        "  --max-new-tokens 80 \\\n",
        "  --max-input-tokens 1536 \\\n",
        "  --temperature 0.0 \\\n",
        "  --use-8bit  # Use 8-bit quantization for memory efficiency\n",
        "\n",
        "# Phase 2: Judge the generated candidates\n",
        "# This creates preference pairs in prefs/preferences_<signature>.jsonl\n",
        "!python make_prefs.py --phase judge \\\n",
        "  --judge-model Qwen/Qwen2.5-3B-Instruct \\\n",
        "  --data-path data/processed/train.jsonl \\\n",
        "  --max-samples 600 \\\n",
        "  --n-generations 2 \\\n",
        "  --judge-batch-size 12 \\\n",
        "  --min-margin 0.12 \\\n",
        "  --judge-use-8bit \\\n",
        "  --output-mode rich  # Include parsed outputs and metadata\n",
        "\n",
        "# Check the generated preference data\n",
        "!echo \"\\nPreference data statistics:\"\n",
        "!ls -lh prefs/\n",
        "!wc -l prefs/preferences_*.jsonl\n",
        "!echo \"\\nSample preference pair:\"\n",
        "!head -n 1 prefs/preferences_*.jsonl | python -m json.tool | head -30"
      ],
      "metadata": {
        "id": "4T3vPMLidBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train Reward Model\n",
        "\n",
        "Train a reward model on the preference pairs using Bradley-Terry loss.\n",
        "\n",
        "### Key Features:\n",
        "- **Anti-gaming penalties**: Penalizes invalid citations, over-citation, and excessive length\n",
        "- **Support span checking**: Ensures cited evidence actually contains answer support\n",
        "- **Confidence calibration**: Rewards appropriate confidence levels\n",
        "\n",
        "### Training Details:\n",
        "- Base model frozen, only reward head trained\n",
        "- Validation includes AUC, calibration, and baseline comparison\n",
        "- Best model saved based on ROC-AUC score"
      ],
      "metadata": {
        "id": "rm-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create reward model configuration\n",
        "rm_config = \"\"\"base_model: google/gemma-2-9b\n",
        "output_dir: checkpoints/rm\n",
        "learning_rate: 5e-6\n",
        "batch_size: 16\n",
        "eval_batch_size: 32\n",
        "num_epochs: 3\n",
        "warmup_ratio: 0.1\n",
        "weight_decay: 0.01\n",
        "max_seq_length: 2048\n",
        "gradient_accumulation_steps: 1\n",
        "eval_steps: 100\n",
        "save_steps: 200\n",
        "logging_steps: 10\n",
        "load_in_8bit: false\n",
        "seed: 42\n",
        "\n",
        "# Anti-gaming penalty weights (tune these based on your needs)\n",
        "citation_penalty_weight: 0.2      # Invalid citations penalty\n",
        "confidence_penalty_weight: 0.1     # Confidence mismatch penalty\n",
        "length_penalty_weight: 0.05        # Excessive length penalty\n",
        "overcitation_penalty_weight: 0.15  # Citing too many chunks\n",
        "span_penalty_weight: 0.25          # Support span mismatch penalty\n",
        "\"\"\"\n",
        "\n",
        "with open('configs/rm.yaml', 'w') as f:\n",
        "    f.write(rm_config)\n",
        "\n",
        "# Train the reward model\n",
        "!python train_rm.py -c configs/rm.yaml\n",
        "\n",
        "# Validate the trained reward model\n",
        "!python train_rm.py -c configs/rm.yaml --validate-only --checkpoint best\n",
        "\n",
        "# Display training history\n",
        "!echo \"\\nTraining History:\"\n",
        "!cat checkpoints/rm/training_history.json | python -m json.tool | tail -50"
      ],
      "metadata": {
        "id": "3VAeWdHcjI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Analysis and Next Steps\n",
        "\n",
        "### Analyze Results\n",
        "View the evaluation metrics and training curves to assess model performance."
      ],
      "metadata": {
        "id": "analysis-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and visualize results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load evaluation results\n",
        "with open('eval_results.json', 'r') as f:\n",
        "    eval_data = json.load(f)\n",
        "\n",
        "# Display improvements\n",
        "if 'deltas' in eval_data:\n",
        "    print(\"Improvements over base model:\")\n",
        "    print(f\"  EM improvement: {eval_data['deltas']['em']:.3f}\")\n",
        "    print(f\"  F1 improvement: {eval_data['deltas']['f1']:.3f}\")\n",
        "    print(f\"  Hallucination reduction: {eval_data['deltas']['hallucination_rate']:.3f}\")\n",
        "    print(f\"  Citation improvement: {eval_data['deltas']['citation_correctness']:.3f}\")\n",
        "\n",
        "# Load reward model training history\n",
        "with open('checkpoints/rm/training_history.json', 'r') as f:\n",
        "    rm_history = json.load(f)\n",
        "\n",
        "# Plot reward model training curve\n",
        "if rm_history['train_losses']:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rm_history['train_losses'])\n",
        "    plt.title('Reward Model Training Loss')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    \n",
        "    if rm_history['eval_metrics']:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        eval_aucs = [m.get('roc_auc', 0) for m in rm_history['eval_metrics']]\n",
        "        plt.plot(eval_aucs)\n",
        "        plt.title('Reward Model Validation AUC')\n",
        "        plt.xlabel('Evaluation Step')\n",
        "        plt.ylabel('ROC-AUC')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "analysis-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "With the trained SFT model and reward model, you can:\n",
        "\n",
        "1. **Deploy for Inference**: Use the SFT model for evidence-grounded QA\n",
        "2. **Further RLHF Training**: Use the reward model for PPO training\n",
        "3. **DPO Training**: Use preference pairs directly for Direct Preference Optimization\n",
        "4. **Iterative Improvement**: Generate new preferences from improved models\n",
        "\n",
        "### Saving Models to Hub\n",
        "\n",
        "To share your models on Hugging Face Hub:\n",
        "```python\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"checkpoints/sft/gemma2-9b/checkpoint-final\",\n",
        "    repo_id=\"your-username/factuality-gemma-9b-sft\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "```\n",
        "\n",
        "### Tips for Production\n",
        "\n",
        "- **Inference Optimization**: Use Flash Attention 2 and torch.compile for faster inference\n",
        "- **Batching**: Implement dynamic batching for production serving\n",
        "- **Monitoring**: Track citation accuracy and hallucination rates in production\n",
        "- **Continuous Learning**: Collect user feedback for iterative improvement"
      ],
      "metadata": {
        "id": "next-steps"
      }
    }
  ]
}