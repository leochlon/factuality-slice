# configs/sft_gemma2.yaml
model_name: google/gemma-2-9b
output_dir: checkpoints/sft/gemma2-9b
trust_remote_code: true
load_in_4bit: false
bf16: true
attn_implementation: flash_attention_2

lora:
  r: 32
  alpha: 64
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

data:
  train_path: data/processed/train.jsonl
  val_path: data/processed/val.jsonl
  template: default

training:
  max_seq_length: 2048
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 64
  num_train_epochs: 1
  learning_rate: 1.5e-5
  weight_decay: 0.1
  warmup_ratio: 0.03
  logging_steps: 10
  eval_steps: 5
  save_steps: 5
  save_total_limit: 2
  lr_scheduler_type: cosine
  packing: true
  seed: 42